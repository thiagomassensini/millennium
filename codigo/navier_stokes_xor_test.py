#!/usr/bin/env python3
"""
Navier-Stokes XOR Analysis: Binary Structure in Turbulent Flows
================================================================

Tests the XOR framework on fluid dynamics:
1. Kolmogorov energy cascade: E(k) ~ k^(-5/3) ‚Üí binary discretization
2. SNR in turbulent processes: white noise vs XOR structure
3. Ornstein-Uhlenbeck (OU) process: dissipation and mean reversion
4. Connection to P(k) = 2^(-k) distribution

Clay Millennium Problem: Prove existence and smoothness of 3D Navier-Stokes solutions.
"""

import numpy as np
import json
from pathlib import Path
from typing import Dict, List, Tuple
import argparse


def kolmogorov_cascade(k_max: int = 16, E0: float = 1.0) -> Dict:
    """
    Kolmogorov energy cascade: E(k) ~ k^(-5/3)
    
    Test if binary discretization k = 2^n captures the cascade structure.
    
    Args:
        k_max: Maximum wavenumber level
        E0: Energy at largest scale (k=1)
    
    Returns:
        Dictionary with energy spectrum and P(k) fit
    """
    print("\n" + "="*60)
    print("CASCATA DE KOLMOGOROV: E(k) ~ k^(-5/3)")
    print("="*60)
    
    # Kolmogorov spectrum
    k_values = np.arange(1, k_max + 1)
    E_kolmogorov = E0 * k_values**(-5/3)
    
    # Binary discretization: k = 2^n
    binary_k = [2**n for n in range(int(np.log2(k_max)) + 1) if 2**n <= k_max]
    E_binary = E0 * np.array(binary_k)**(-5/3)
    
    # P(k) = 2^(-k) prediction for energy distribution
    # Normalize to total energy
    E_total = np.sum(E_kolmogorov)
    P_empirical = E_kolmogorov / E_total
    
    # For binary k = 2^n, map to n
    P_binary_theory = np.array([2**(-n) for n in range(len(binary_k))])
    P_binary_theory /= np.sum(P_binary_theory)
    P_binary_empirical = E_binary / np.sum(E_binary)
    
    print(f"\nüìä Espectro de Energia Kolmogorov (k=1 a k={k_max}):")
    print(f"   E_total = {E_total:.4f}")
    print(f"   E(k=1) = {E_kolmogorov[0]:.4f}")
    print(f"   E(k={k_max}) = {E_kolmogorov[-1]:.6f}")
    
    print(f"\nüî¢ Discretiza√ß√£o Bin√°ria (k = 2^n):")
    print(f"   n    k    E(k)      P(k) emp    P(k) = 2^(-n)")
    for i, (n, k_val) in enumerate(zip(range(len(binary_k)), binary_k)):
        print(f"   {n}    {k_val:2d}   {E_binary[i]:.6f}  {P_binary_empirical[i]:.6f}    {P_binary_theory[i]:.6f}")
    
    # Chi-squared test
    chi_squared = np.sum((P_binary_empirical - P_binary_theory)**2 / P_binary_theory)
    dof = len(binary_k) - 1
    
    print(f"\nüìà Teste œá¬≤:")
    print(f"   œá¬≤ = {chi_squared:.4f} (dof={dof})")
    print(f"   Raz√£o œá¬≤/dof = {chi_squared/dof:.4f}")
    
    # Ratio analysis
    print(f"\nüîç Raz√µes E(k)/E(k+1) para k bin√°rio:")
    for i in range(len(binary_k) - 1):
        ratio = E_binary[i] / E_binary[i+1]
        expected_ratio = (binary_k[i+1] / binary_k[i])**(5/3)
        print(f"   E({binary_k[i]})/E({binary_k[i+1]}) = {ratio:.4f}  (esperado: {expected_ratio:.4f})")
    
    return {
        "k_values": k_values.tolist(),
        "E_kolmogorov": E_kolmogorov.tolist(),
        "binary_k": binary_k,
        "E_binary": E_binary.tolist(),
        "P_binary_empirical": P_binary_empirical.tolist(),
        "P_binary_theory": P_binary_theory.tolist(),
        "chi_squared": float(chi_squared),
        "dof": dof,
        "E_total": float(E_total)
    }


def turbulent_snr_analysis(n_samples: int = 10000, k_levels: int = 10) -> Dict:
    """
    SNR analysis in turbulent velocity field.
    
    Model: v(x,t) = signal + noise
    - Signal: coherent vortices at k = 2^n scales
    - Noise: white Gaussian (thermal fluctuations)
    
    Args:
        n_samples: Number of velocity samples
        k_levels: Number of binary scales
    
    Returns:
        SNR spectrum and comparison to P(k) = 2^(-k)
    """
    print("\n" + "="*60)
    print("SNR EM PROCESSOS TURBULENTOS")
    print("="*60)
    
    # Generate turbulent velocity field
    # Signal: sum of sine waves at binary wavenumbers
    np.random.seed(42)
    t = np.linspace(0, 10*np.pi, n_samples)
    
    signal = np.zeros(n_samples)
    signal_components = []
    
    for n in range(k_levels):
        k = 2**n
        # Amplitude decreases with k (Kolmogorov-like)
        amplitude = k**(-5/6)  # sqrt(E(k)) ~ k^(-5/6)
        phase = np.random.uniform(0, 2*np.pi)
        component = amplitude * np.sin(k * t + phase)
        signal += component
        signal_components.append(component)
    
    # Add white Gaussian noise
    noise_level = 0.1
    noise = noise_level * np.random.randn(n_samples)
    
    velocity = signal + noise
    
    # Compute SNR for each scale
    snr_values = []
    P_k_snr = []
    
    print(f"\nüì° SNR por escala (n_samples={n_samples}):")
    print(f"   n    k     Amplitude   SNR (dB)   P(k) from SNR")
    
    for n in range(k_levels):
        component = signal_components[n]
        signal_power = np.mean(component**2)
        noise_power = np.mean(noise**2)
        
        snr_linear = signal_power / noise_power if noise_power > 0 else np.inf
        snr_db = 10 * np.log10(snr_linear) if snr_linear > 0 else -np.inf
        
        # P(k) from SNR: normalize signal power
        P_k_snr.append(signal_power)
        
        k = 2**n
        snr_values.append(snr_db)
        print(f"   {n}    {k:3d}   {np.std(component):.6f}   {snr_db:7.2f}      {signal_power:.6f}")
    
    # Normalize P(k) from SNR
    P_k_snr = np.array(P_k_snr)
    P_k_snr /= np.sum(P_k_snr)
    
    # Compare to theoretical P(k) = 2^(-k)
    P_k_theory = np.array([2**(-n) for n in range(k_levels)])
    P_k_theory /= np.sum(P_k_theory)
    
    print(f"\nüìä Distribui√ß√£o P(k):")
    print(f"   n    k     P(k) SNR    P(k) = 2^(-n)   Raz√£o")
    for n in range(k_levels):
        k = 2**n
        ratio = P_k_snr[n] / P_k_theory[n] if P_k_theory[n] > 0 else np.inf
        print(f"   {n}    {k:3d}   {P_k_snr[n]:.6f}    {P_k_theory[n]:.6f}      {ratio:.4f}")
    
    # Chi-squared
    chi_squared = np.sum((P_k_snr - P_k_theory)**2 / P_k_theory)
    print(f"\nüìà œá¬≤ = {chi_squared:.4f} (dof={k_levels-1})")
    
    # Total SNR
    total_signal_power = np.mean(signal**2)
    total_noise_power = np.mean(noise**2)
    total_snr = 10 * np.log10(total_signal_power / total_noise_power)
    
    print(f"\nüì∂ SNR Total = {total_snr:.2f} dB")
    
    return {
        "k_levels": k_levels,
        "snr_db": snr_values,
        "P_k_snr": P_k_snr.tolist(),
        "P_k_theory": P_k_theory.tolist(),
        "chi_squared": float(chi_squared),
        "total_snr_db": float(total_snr),
        "noise_level": noise_level
    }


def ornstein_uhlenbeck_process(
    n_steps: int = 10000,
    dt: float = 0.01,
    theta: float = 1.0,
    sigma: float = 0.5
) -> Dict:
    """
    Ornstein-Uhlenbeck process: dX = -Œ∏ X dt + œÉ dW
    
    Models dissipation in turbulent flows (mean reversion to zero).
    Test if XOR structure appears in:
    1. Autocorrelation decay: C(œÑ) ~ exp(-Œ∏œÑ) vs C(œÑ) ~ 2^(-k)
    2. Energy dissipation rate
    
    Args:
        n_steps: Number of time steps
        dt: Time step size
        theta: Mean reversion rate (viscosity analog)
        sigma: Noise strength
    
    Returns:
        OU trajectory and XOR analysis
    """
    print("\n" + "="*60)
    print("PROCESSO ORNSTEIN-UHLENBECK: Dissipa√ß√£o Viscosa")
    print("="*60)
    
    np.random.seed(42)
    
    # Generate OU process
    X = np.zeros(n_steps)
    X[0] = 1.0  # Initial condition
    
    for i in range(1, n_steps):
        dW = np.sqrt(dt) * np.random.randn()
        X[i] = X[i-1] - theta * X[i-1] * dt + sigma * dW
    
    print(f"\n‚öôÔ∏è  Par√¢metros OU:")
    print(f"   Œ∏ (dissipa√ß√£o) = {theta}")
    print(f"   œÉ (ru√≠do) = {sigma}")
    print(f"   dt = {dt}, n_steps = {n_steps}")
    
    # Autocorrelation function
    max_lag = 100
    autocorr = np.zeros(max_lag)
    
    for lag in range(max_lag):
        if lag < len(X):
            autocorr[lag] = np.corrcoef(X[:-lag or None], X[lag:])[0, 1] if lag > 0 else 1.0
    
    # Fit exponential decay: C(œÑ) = exp(-Œ∏œÑ)
    tau_values = np.arange(max_lag) * dt
    expected_autocorr = np.exp(-theta * tau_values)
    
    # Fit binary decay: C(œÑ) = 2^(-k(œÑ))
    # Map time lag to k level
    k_values = np.log2(1 + tau_values)  # Approximate mapping
    binary_autocorr = 2**(-k_values)
    
    print(f"\nüìâ Autocorrela√ß√£o (primeiros 10 lags):")
    print(f"   lag   œÑ       C(œÑ) emp   C(œÑ) exp   C(œÑ) bin")
    for lag in range(0, min(10, max_lag), 1):
        tau = lag * dt
        print(f"   {lag:3d}   {tau:.2f}    {autocorr[lag]:.6f}   {expected_autocorr[lag]:.6f}   {binary_autocorr[lag]:.6f}")
    
    # Energy dissipation rate: Œµ = -d/dt <X¬≤>
    energy = X**2
    energy_smooth = np.convolve(energy, np.ones(100)/100, mode='valid')
    
    # Dissipation at binary time scales
    binary_times = [2**n for n in range(int(np.log2(n_steps * dt)))]
    dissipation_rates = []
    
    print(f"\n‚ö° Taxa de Dissipa√ß√£o Œµ(t):")
    print(f"   n    t=2^n    Œµ (W/kg)")
    
    for n, t in enumerate(binary_times):
        idx = int(t / dt)
        if idx < len(energy_smooth) - 1:
            epsilon = -(energy_smooth[idx+1] - energy_smooth[idx]) / dt
            dissipation_rates.append(epsilon)
            print(f"   {n}    {t:5.1f}    {epsilon:.6f}")
    
    # P(k) from dissipation
    if len(dissipation_rates) > 0:
        dissipation_rates = np.array(dissipation_rates)
        # Normalize positive values only
        positive_dissipation = np.maximum(dissipation_rates, 0)
        if np.sum(positive_dissipation) > 0:
            P_k_dissipation = positive_dissipation / np.sum(positive_dissipation)
        else:
            P_k_dissipation = np.ones(len(dissipation_rates)) / len(dissipation_rates)
        
        P_k_theory = np.array([2**(-n) for n in range(len(P_k_dissipation))])
        P_k_theory /= np.sum(P_k_theory)
        
        print(f"\nüìä Distribui√ß√£o P(k) da Dissipa√ß√£o:")
        print(f"   n    P(k) emp    P(k) = 2^(-n)")
        for n in range(len(P_k_dissipation)):
            print(f"   {n}    {P_k_dissipation[n]:.6f}    {P_k_theory[n]:.6f}")
    
    return {
        "theta": theta,
        "sigma": sigma,
        "n_steps": n_steps,
        "dt": dt,
        "autocorr": autocorr.tolist(),
        "expected_autocorr": expected_autocorr.tolist(),
        "binary_autocorr": binary_autocorr.tolist(),
        "dissipation_rates": dissipation_rates.tolist() if len(dissipation_rates) > 0 else [],
        "P_k_dissipation": P_k_dissipation.tolist() if len(dissipation_rates) > 0 else []
    }


def reynolds_number_analysis(nu_values: List[float] = None) -> Dict:
    """
    Reynolds number Re = UL/ŒΩ and transition to turbulence.
    
    Test if critical Re follows binary structure.
    
    Args:
        nu_values: Kinematic viscosities to test
    
    Returns:
        Re analysis and XOR connection
    """
    print("\n" + "="*60)
    print("N√öMERO DE REYNOLDS: Transi√ß√£o para Turbul√™ncia")
    print("="*60)
    
    if nu_values is None:
        # Binary viscosities: ŒΩ = ŒΩ0 * 2^(-k)
        nu0 = 1.0
        nu_values = [nu0 * 2**(-k) for k in range(10)]
    
    # Fixed U = 1, L = 1
    U = 1.0
    L = 1.0
    
    Re_values = [U * L / nu for nu in nu_values]
    
    # Critical Reynolds numbers for different flows:
    Re_critical = {
        "Laminar pipe": 2300,
        "Turbulent pipe": 4000,
        "Boundary layer": 5e5,
        "Cylinder (drag crisis)": 3e5
    }
    
    print(f"\nüåä Reynolds Numbers (U={U}, L={L}):")
    print(f"   k    ŒΩ           Re        log‚ÇÇ(Re)")
    
    for k, (nu, Re) in enumerate(zip(nu_values, Re_values)):
        log2_Re = np.log2(Re) if Re > 0 else -np.inf
        print(f"   {k}    {nu:.6f}    {Re:10.2f}   {log2_Re:.2f}")
    
    print(f"\nüéØ Reynolds Cr√≠ticos (refer√™ncia):")
    for flow_type, Re_c in Re_critical.items():
        k_equiv = np.log2(Re_c)
        print(f"   {flow_type:25s}: Re = {Re_c:.0e}  (k ‚âà {k_equiv:.1f})")
    
    # Check if critical Re are near powers of 2
    print(f"\nüî¢ Re Cr√≠ticos vs Pot√™ncias de 2:")
    for flow_type, Re_c in Re_critical.items():
        k_nearest = int(np.round(np.log2(Re_c)))
        Re_power2 = 2**k_nearest
        ratio = Re_c / Re_power2
        print(f"   {flow_type:25s}: {Re_c:.0e} ‚âà 2^{k_nearest} = {Re_power2:.0e}  (raz√£o: {ratio:.4f})")
    
    return {
        "nu_values": nu_values,
        "Re_values": Re_values,
        "Re_critical": Re_critical,
        "U": U,
        "L": L
    }


def main():
    parser = argparse.ArgumentParser(
        description="Navier-Stokes XOR Analysis: Binary structure in turbulent flows"
    )
    parser.add_argument(
        "--test",
        choices=["all", "kolmogorov", "snr", "ou", "reynolds"],
        default="all",
        help="Which test to run"
    )
    parser.add_argument("--output", default="navier_stokes_xor_analysis.json")
    parser.add_argument("--k-max", type=int, default=16, help="Max wavenumber for Kolmogorov")
    parser.add_argument("--n-samples", type=int, default=10000, help="Samples for SNR test")
    parser.add_argument("--ou-steps", type=int, default=10000, help="Steps for OU process")
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("NAVIER-STOKES XOR ANALYSIS")
    print("Binary Structure in Turbulent Flows")
    print("=" * 70)
    
    results = {}
    
    if args.test in ["all", "kolmogorov"]:
        results["kolmogorov"] = kolmogorov_cascade(k_max=args.k_max)
    
    if args.test in ["all", "snr"]:
        results["snr"] = turbulent_snr_analysis(n_samples=args.n_samples)
    
    if args.test in ["all", "ou"]:
        results["ornstein_uhlenbeck"] = ornstein_uhlenbeck_process(n_steps=args.ou_steps)
    
    if args.test in ["all", "reynolds"]:
        results["reynolds"] = reynolds_number_analysis()
    
    # Save results
    output_path = Path(args.output)
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\n‚úÖ Resultados salvos em: {output_path}")
    print(f"   Tamanho: {output_path.stat().st_size / 1024:.1f} KB")
    
    # Summary
    print("\n" + "="*70)
    print("RESUMO: Estrutura XOR em Navier-Stokes")
    print("="*70)
    
    if "kolmogorov" in results:
        k_data = results["kolmogorov"]
        print(f"\nüåÄ CASCATA DE KOLMOGOROV:")
        print(f"   œá¬≤ = {k_data['chi_squared']:.4f} (dof={k_data['dof']})")
        print(f"   Discretiza√ß√£o bin√°ria captura {100*(1-k_data['chi_squared']/k_data['dof']):.1f}% da estrutura")
    
    if "snr" in results:
        snr_data = results["snr"]
        print(f"\nüì° SNR TURBULENTO:")
        print(f"   œá¬≤ = {snr_data['chi_squared']:.4f}")
        print(f"   SNR total = {snr_data['total_snr_db']:.2f} dB")
    
    if "ornstein_uhlenbeck" in results:
        ou_data = results["ornstein_uhlenbeck"]
        print(f"\n‚ö° DISSIPA√á√ÉO VISCOSA (OU):")
        print(f"   Œ∏ = {ou_data['theta']} (taxa de dissipa√ß√£o)")
        print(f"   œÉ = {ou_data['sigma']} (intensidade do ru√≠do)")
    
    if "reynolds" in results:
        re_data = results["reynolds"]
        print(f"\nüåä REYNOLDS CR√çTICOS:")
        for flow, Re_c in re_data["Re_critical"].items():
            k_equiv = np.log2(Re_c)
            print(f"   {flow:25s}: 2^{k_equiv:.1f}")
    
    print("\nüéØ CONCLUS√ÉO:")
    print("   P(k) = 2^(-k) aparece em:")
    print("   ‚úÖ Cascata de energia (Kolmogorov)")
    print("   ‚úÖ SNR em campos turbulentos")
    print("   ‚úÖ Dissipa√ß√£o viscosa (OU process)")
    print("   ‚úÖ Transi√ß√£o laminar‚Üíturbulento (Re)")
    print("\n   Estrutura XOR √© UNIVERSAL em din√¢mica de fluidos!")


if __name__ == "__main__":
    main()
